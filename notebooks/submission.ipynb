# ====================================================
# ## Setup and Imports
# ====================================================
import os
import sys
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision.models import vit_b_16
from transformers import BertModel, AutoTokenizer

import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.neighbors import NearestNeighbors

# Add local path for PyTorch Metric Learning library if provided as a dataset
# This is necessary for offline Kaggle environments.
sys.path.insert(0, '/kaggle/input/pml-source-code')

# ====================================================
# ## Model Definition
# ====================================================
class ShopeeTransformer(nn.Module):
    """
    A multi-modal model combining a Vision Transformer (ViT) for images and
    BERT for text titles to generate a single embedding for product matching.
    """
    def __init__(self,
                 vit_weights_path,
                 bert_model_path,
                 embed_dim=512):
        super().__init__()
        
        # --- Image Branch (Vision Transformer) ---
        self.image_encoder = vit_b_16(weights=None)
        # Load pre-trained weights locally for offline use
        self.image_encoder.load_state_dict(torch.load(vit_weights_path, map_location="cpu"))
        # Remove the final classification head to use ViT as a feature extractor
        self.image_encoder.heads = nn.Identity()
        self.image_proj = nn.Linear(768, embed_dim) # Project ViT's output to the common embedding dimension

        # --- Text Branch (BERT) ---
        self.text_encoder = BertModel.from_pretrained(bert_model_path, local_files_only=True)
        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, embed_dim) # Project BERT's output

        # --- Fusion Layer ---
        # A final linear layer to fuse the concatenated image and text features
        self.fusion_proj = nn.Linear(embed_dim * 2, embed_dim)

    def forward(self, image, input_ids, attention_mask):
        """
        Forward pass to generate a normalized embedding.
        """
        # 1. Get image features
        image_feat = self.image_encoder(image)  # Shape: [batch_size, 768]
        image_feat = self.image_proj(image_feat) # Shape: [batch_size, embed_dim]

        # 2. Get text features
        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        # Use the embedding of the [CLS] token for sentence representation
        text_feat = text_outputs.last_hidden_state[:, 0, :]
        text_feat = self.text_proj(text_feat) # Shape: [batch_size, embed_dim]

        # 3. Concatenate and fuse features
        fused = torch.cat([image_feat, text_feat], dim=1) # Shape: [batch_size, 2 * embed_dim]
        fused = self.fusion_proj(fused) # Shape: [batch_size, embed_dim]

        # 4. L2 Normalize the final embedding (crucial for metric learning)
        fused = F.normalize(fused, p=2, dim=1)

        return fused

# ====================================================
# ## Dataset and Transforms
# ====================================================
class ShopeeDataset(Dataset):
    """
    Custom PyTorch Dataset for loading Shopee products (image and title).
    """
    def __init__(self, dataframe, transform, tokenizer, max_length=128):
        self.data = dataframe.reset_index(drop=True)
        self.transform = transform
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        # Load image
        image = Image.open(row['image_path']).convert('RGB')
        
        # Apply transformations
        if self.transform:
            image_np = np.array(image)
            image = self.transform(image=image_np)['image']

        # Tokenize title
        encoding = self.tokenizer(
            row['title'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'image': image,
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0)
        }

def get_transforms(image_size=224):
    """
    Returns albumentations transforms for inference.
    """
    return A.Compose([
        A.Resize(image_size, image_size),
        # Normalize to [-1, 1], a common range for ViT
        A.Normalize(mean=0.5, std=0.5),
        ToTensorV2(),
    ])

# ====================================================
# ## Inference Functions
# ====================================================
def get_embeddings(model, data_loader, device):
    """
    Generates embeddings for all items in a dataloader.
    """
    model.eval()
    all_embeddings = []
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Generating Embeddings"):
            # Move data to the specified device
            images = batch['image'].to(device)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            embeddings = model(images, input_ids, attention_mask)
            all_embeddings.append(embeddings.cpu().numpy())
            
    return np.concatenate(all_embeddings, axis=0)

def find_matches(df, embeddings, k_neighbors, similarity_threshold):
    """
    Finds matching products by finding nearest neighbors in the embedding space
    and filtering them by a similarity threshold.
    """
    num_samples = len(embeddings)
    # Ensure k is not larger than the number of items in the dataset
    if k_neighbors > num_samples:
        print(f"Warning: k_neighbors ({k_neighbors}) > num_samples ({num_samples}). Adjusting to {num_samples}.")
        k_neighbors = num_samples
        
    # Use scikit-learn's NearestNeighbors for efficient search with cosine distance
    nn_model = NearestNeighbors(n_neighbors=k_neighbors, metric='cosine', n_jobs=-1)
    nn_model.fit(embeddings)
    
    distances, indices = nn_model.kneighbors(embeddings)
    
    predictions = []
    posting_ids = df['posting_id'].values # Use .values for faster access
    
    for i in tqdm(range(num_samples), desc="Finding Matches"):
        # Cosine Similarity = 1 - Cosine Distance
        confident_indices = indices[i][distances[i] < (1 - similarity_threshold)]
        
        # Get the posting_ids for the confident matches
        match_ids = posting_ids[confident_indices]
        
        predictions.append(" ".join(match_ids))
        
    df['matches'] = predictions
    return df

# ====================================================
# ## Main Execution
# ====================================================
def main():
    # --- Configuration ---
    config = {
        'model_path': '/kaggle/input/shopee-model-weights/best_model_fold_4.pt',
        'vit_weights_path': "/kaggle/input/vit-weights/vit_b_16-c867db91.pth",
        'bert_model_path': "/kaggle/input/bert-model/bert-base-uncased",
        'image_dir': '/kaggle/input/shopee-product-matching/test_images',
        'tabular_data_path': '/kaggle/input/shopee-product-matching/test.csv',
        'output_file': 'submission.csv',

        'image_size': 224,
        'batch_size': 128,
        'embedding_size': 512,
        'max_text_length': 128,
        'device': torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        
        # --- Post-processing Parameters ---
        'k_neighbors': 50,           # Max group size is 50
        'similarity_threshold': 0.85 # Confidence threshold for a match
    }
    print(f"Using device: {config['device']}")

    # --- 1. Load Model and Tokenizer ---
    print("Loading model and tokenizer...")
    model = ShopeeTransformer(
        vit_weights_path=config['vit_weights_path'],
        bert_model_path=config['bert_model_path'],
        embed_dim=config['embedding_size']
    )
    model.load_state_dict(torch.load(config['model_path'], map_location=config['device']))
    model.to(config['device'])
    
    tokenizer = AutoTokenizer.from_pretrained(config['bert_model_path'], local_files_only=True)
    print("Model and tokenizer loaded.")

    # --- 2. Prepare Data ---
    print("Preparing test data...")
    df_test = pd.read_csv(config['tabular_data_path'])
    # Handle case where test set is small
    if len(df_test) == 3:
        config['k_neighbors'] = 3
    
    df_test['image_path'] = df_test['image'].apply(lambda x: os.path.join(config['image_dir'], x))
    
    test_dataset = ShopeeDataset(
        df_test, 
        transform=get_transforms(config['image_size']),
        tokenizer=tokenizer,
        max_length=config['max_text_length']
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )
    print("Data preparation complete.")

    # --- 3. Generate Embeddings ---
    test_embeddings = get_embeddings(model, test_loader, config['device'])

    # --- 4. Find Matches ---
    submission_df = find_matches(
        df_test, 
        test_embeddings, 
        k_neighbors=config['k_neighbors'], 
        similarity_threshold=config['similarity_threshold']
    )

    # --- 5. Create Submission File ---
    submission_df[['posting_id', 'matches']].to_csv(config['output_file'], index=False)
    print(f"âœ… Submission file created successfully at: {config['output_file']}")
    print(submission_df[['posting_id', 'matches']].head())

if __name__ == "__main__":
    main()
